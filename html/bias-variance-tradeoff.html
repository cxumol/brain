<div id="content"><p><img src="https://miro.medium.com/max/738/1*wqDhhG2BjkBCl5WuHojddw.png" alt="" /></p>

<p>The <strong>bias-variance tradeoff</strong> is the property where a predictive model with a <em>low <a href="bias-of-estimators">bias</a></em> has a <em>higher <a href="variance">variance</a></em> and vice versa. </p>

<ul>
<li>High <a href="bias-of-estimators">bias</a> can cause a model to miss important relations between features and target outputs.</li>
<li>High <a href="variance">variance</a> can lead to the random noise of the <a href="training-set">training set</a> to be modeled, namely <a href="overfitting">overfitting</a>. This, in turn, will likely lead to greater errors when introduced to <a href="training-set">new data</a>.</li>
</ul>

<p>An often used simplification of this tradeoff reads:</p>

<blockquote>
  <p>Bias is related with a model failing to fit the training set and variance is related with a model failing to fit the testing set.</p>
</blockquote>

<div class="backlinks">

<h2 id="backlinks">Backlinks</h2>

<ul>
<li><a href="ridge-regression">Ridge regression</a></li>
<li><a href="stk-in4300-lecture-03---more-model-assessment-and-shrinkage-methods">STK-IN4300 Lecture 03 - More model assessment and shrinkage methods</a></li>
</ul>

</div>
</div>