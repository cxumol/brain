<div id="content"><p><strong>Gradient descent</strong> describes the process of finding a local minimum of a function by following the negative value of the gradient at each point stepwise. Notationally, this is described the following way:</p>

<p>$$a_{n+1} = a_n  + \eta \nabla f(a_n)$$</p>

<p>Here, $a_i$ refers to the $i$'th step, $\eta$ is the step size and $f$ is the function in question.</p>

<div class="backlinks">

<h2 id="backlinks">Backlinks</h2>

<ul>
<li><a href="stochastic-gradient-descent">Stochastic gradient descent</a></li>
</ul>

</div>
</div>