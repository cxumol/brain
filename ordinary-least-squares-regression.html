<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet"> 
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet"> 

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">

  <title>Ordinary least squares regression</title>
  <link rel="icon" href="https://raw.githubusercontent.com/kmaasrud/brain/master/favicon.svg" type="image/svg+xml">
  <link rel="alternate icon" href="https://raw.githubusercontent.com/kmaasrud/brain/master/favicon.ico" type="image/x-icon">
</head>

<body>
  <div class="grid">
    <nav>
      <ul>
        <a href="https://www.kmaasrud.com/brain"><img id="logo" src="https://raw.githubusercontent.com/kmaasrud/brain/master/brain.svg"></img></a>
        <br>
        <li><a href="https://www.kmaasrud.com/projects">Projects</a></li>
        <li><a href="https://github.com/kmaasrud">GitHub</a></li>
        <br>
      </ul>
      <div class="footer">
        <p>Get in touch with me:</p>
        <a href="mailto:km@aasrud.com">km@aasrud.com</a>
      </div>
    </nav>
    <article>
      <header>Ordinary least squares regression</header>
      <div id="content"><p><strong>Ordinary least squares</strong> (OLS) is a method of estimating the parameters in a <a href="linear-regression">linear regression</a> model. </p>

<p>We have a dataset of $n$ observations $(y_i, \mathbf x_i)$, where $\mathbf x_i$ are the column vectors of length $p$ containing the parameters and $y_i$ are the scalar responses. We set up the linear function for the $i$'th case as</p>

<p>$$ y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_i,$$</p>

<p>$$ y_i = \mathbf x^T_i\beta + \epsilon_i .$$</p>

<p>Here $\beta$ is an unknown $p\times 1$ vector and $\epsilon_i$ is the error of the model in the $i$'th case. So for all $i$ cases, we write the total equation on matrix form as</p>

<p>$$ \mathbf y = \mathbf X\mathbf \beta + \mathbf \epsilon ,$$</p>

<p>with $\mathbf X$ being an $n\times p$ matrix, often called the <em><a href="design-matrix">design matrix</a></em> or <em>matrix of regressors</em>. It's worth noting that - contrary to what on might've thought - the $i$'th <strong>row</strong> of $\mathbf X$ equals $\mathbf x_i^T$. Also, the constant term is included in our equation by setting $x_{i1} = 1\ \forall\ i=1,\dots, n$.</p>

<p>Now because of the errors, $\mathbf y = \mathbf X\mathbf \beta$ does not have a unique solution. However, we can find the best fit by finding the vector $\hat \beta$ that minimizes some sort of <a href="cost-function">cost function</a>,</p>

<p>$$ \hat \beta = \underset{\beta}{\arg \min} S(\beta) .$$</p>

<p>$\hat \beta$ will give us an estimate of $\mathbf y$, namely $\hat{\mathbf y} = \mathbf X \hat \beta$.</p>

<p>In the case of OLS, the <a href="cost-function">cost function</a> we use is the <a href="residual-sum-of-squares">residual sum of squares (RSS) function</a>:</p>

<p>$$ \text{RSS}(\beta) = \sum_{i=1}^N \left(y_i - \hat y_i\right)^2 .$$</p>

<p>Changing into matrix notation again, we get</p>

<p>$$ \text{RSS}(\beta) = (\mathbf y - \hat{\mathbf y})^T(\mathbf y - \hat{\mathbf y}) = (\mathbf y  - \mathbf X\beta)^T(\mathbf y - \mathbf X\beta) ,$$</p>

<p>which we can differentiate with respect to $\beta$ to find the minimum.</p>

<p>$$ \frac{\partial \text{RSS}}{\partial \beta} = -2\mathbf X^T (\mathbf y - \mathbf X\beta) .$$</p>

<p>Assuming <a href="rank-linear-algebra-">full column rank</a> for $\mathbf X$, $(\mathbf X^T \mathbf X)$ is thus <a href="positive-definite">positive definite</a> (and importantly, <a href="invertibility">invertible</a>). Setting the first derivative to $0$, we get</p>

<p>$$ \mathbf X^T(\mathbf y - \mathbf X\beta) = 0$$</p>

<p>$$\Rightarrow \hat \beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf y $$</p>
</div>
      <div class="backlinks">

<ul>
<li><a href="linear-regression">Linear regression</a></li>
<li><a href="ridge-regression">Ridge regression</a></li>
</ul>

</div>

    </article>
  </div>
  
  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>
  
  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>
</body>

</html>
