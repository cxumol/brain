<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;700&amp;display=swap"
    rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" type="text/css" href="main.css">

  <title>STK-IN4300 Lecture 02 - Linear methods for regression</title>
</head>

<body>
  <header>STK-IN4300 Lecture 02 - Linear methods for regression</header>
  <div id="content"><p>(<a href="riccardo-de-bin">Riccardo's</a> lectures are kind of a joke, I'll take my own notes from the <a href="https://web.stanford.edu/%7Ehastie/Papers/ESLII.pdf">book</a>)</p>

<hr />

<h2 id="linear-methods-of-regression">Linear methods of regression</h2>

<p>A <a href="linear-regression">linear regression</a> model assumes that the regression function $E(Y|X)$ is linear in the inputs. </p>

<p>Assume an input vector $X^T = (X_1, X_2, \dots, X_p)$, where we want to predict an output $Y$. If we know that the output should take a linear form or that this is a reasonable approximation, we can use linear regression. The <a href="linear-regression">linear regression model</a> has the form</p>

<p>$$ f(X) = \beta_0 + \sum^p_{j=1}X_j\beta_j ,$$</p>

<p>where the $\beta_j$'s are unknown coefficients or parameters.</p>

<p>Typically, we have a set of training data $(x_1,y_1),\dots, (x_N,y_N)$, where the $x_i = (x_{i1}, x_{i2},\dots, x_{ip})^T$ elements are vectors containing all the feature measurements for the $i$th case. A common method of estimating the regression function is <strong><a href="least-squares">least squares</a></strong>. Here we pick the $\beta$'s based on minimizing the <a href="residual-sum-of-squares">RSS function</a>:</p>

<p>$$ \text{RSS}(\beta) = \sum_{i=1}^N \left(y_i - f(x_i)\right)^2 = \sum_{i=1}^N \left(y_i - \beta_0 - \sum^p_{j=1}x_{ij}\beta_j\right)^2, $$</p>

<p>where $\beta = (\beta_1, \beta_2, \dots, \beta_p)^T$ is the vector of all the coefficients.</p>

<p>Now how do we minimize this? Start by denoting an input matrix $\mathbf X$ (dimensionality $N\times p +1$), where each row is an input vector (with a 1 in the first position, to allow for constants in the model). $\mathbf y$ (dimensionality $N$) are all the outputs we have in our training set. Now the <a href="residual-sum-of-squares">RSS</a> is</p>

<p>$$ \text{RSS}(\beta) = (\mathbf y  - \mathbf X\beta)^T(\mathbf y - \mathbf X\beta) ,$$</p>

<p>which we can differentiate with respect to $\beta$ to find the minimum.</p>

<p>$$ \frac{\partial \text{RSS}}{\partial \beta} = -2\mathbf X^T (\mathbf y - \mathbf X\beta) $$<br />
$$ \frac{\partial^2\text{RSS}}{\partial \beta \partial \beta^T} = 2\mathbf X^T\mathbf X $$</p>

<p>For the moment, we assume that $\mathbf X$ has <a href="rank-linear-algebra-">full column rank</a> and $\mathbf X^T \mathbf X$ is thus <a href="positive-definite">positive definite</a> (and <a href="invertibility">invertible</a>). Setting the first derivative to $0$, we get</p>

<p>$$ \mathbf X^T(\mathbf y - \mathbf X\beta) = 0 \Rightarrow \hat \beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf y $$</p>

<p>and now we easiliy get the predicted values ($\hat y_i = f(x_i)$, gathered in the vector $\hat{\mathbf y}$) as</p>

<p>$$ \hat{\mathbf y} = \mathbf X\hat \beta = \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf y = \mathbf H\mathbf y .$$</p>

<p>We often call $\mathbf H$ the <em>hat matrix</em>.</p>

<p>(There's more to this, but the book gets kinda technical, and I presume this level of detail will come naturally later)</p>

<h2 id="subset-selection">Subset selection</h2>

<p>The estimates produced by <a href="least-squares">least squares</a> are often not satisfactory, for two reasons:</p>

<ul>
<li><strong>Prediction accuracy</strong>: <a href="least-squares">Least squares estimates</a> often have small bias, but large <a href="variance">variance</a>. Shrinking coefficients or setting them equal zero can often improve the accuracy, by sacrificing some bias.</li>
<li><strong>Interpretation</strong>: We would often like to determine the subset of a large number of predictors that has the largest effect. Here, we sacrifice the smaller details to get a practical "bigger picture".</li>
</ul>
</div>

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>
</body>

</html>
