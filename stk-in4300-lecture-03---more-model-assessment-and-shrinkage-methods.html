<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- IBM Plex Mono -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet"> 
  <!-- IBM Plex Sans -->
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,600;0,700;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet"> 

  <!-- CSS (uses absolute weblinks to work around relative path issues) -->
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/main.css">
  <link rel="stylesheet" type="text/css" href="https://www.kmaasrud.com/brain/main.css">

  <title>STK-IN4300 Lecture 03 - More model assessment and shrinkage methods</title>
  <link rel="icon" href="https://raw.githubusercontent.com/kmaasrud/brain/master/favicon.svg" type="image/svg+xml">
  <link rel="alternate icon" href="https://raw.githubusercontent.com/kmaasrud/brain/master/favicon.ico" type="image/x-icon">
</head>

<body>
  <div class="grid">
    <nav>
      <ul>
        <a href="https://www.kmaasrud.com/brain"><img id="logo" src="https://raw.githubusercontent.com/kmaasrud/brain/master/brain.svg"></img></a>
        <br>
        <li><a href="https://www.kmaasrud.com/projects">Projects</a></li>
        <li><a href="https://github.com/kmaasrud">GitHub</a></li>
        <br>
      </ul>
      <div class="footer">
        <p>Get in touch with me:</p>
        <a href="mailto:km@aasrud.com">km@aasrud.com</a>
      </div>
    </nav>
    <article>
      <header>STK-IN4300 Lecture 03 - More model assessment and shrinkage methods</header>
      <div id="content"><h2 id="cross-validation">Cross-validation</h2>

<p><a href="cross-validation">Cross-validation</a> is a <a href="model-assessment">model assessment technique</a> based on using a <a href="training-set">set of training data</a> to prime it and then comparing the predictions given to a <a href="testing-set">validation dataset</a>, to evaluate the models ability to predict new data. This is to avoid issues like <a href="overfitting">overfitting</a> and/or <a href="selection-bias">selection bias</a>. The method directly estimates the expected extra-sample error:</p>

<p>$$ \text{Err} = \text E\left[L(Y, \hat f(X)\right] ,$$</p>

<p>the average <a href="test-error">average generalization error</a> when $\hat f(X)$ is a applied to an independent test sample from the joint distribution of $X$ and $Y$.</p>

<h3 id="k-fold-cross-validation">$K$-fold cross-validation</h3>

<p>Ideally, we would split our data into a <a href="training-set">training set</a> and a <a href="testing-set">validation set</a>, like explained in <a href="stk-in4300-lecture-02---linear-methods-for-regression">a previous lecture</a>. However, in many cases our dataset is too small for this. In this case, we will have to use <a href="-k--fold-cross-validation">$K$-fold cross-validation</a> to reuse our <a href="training-set">training set</a> as our <a href="testing-set">validation set</a>.</p>

<p>The method involves splitting our dataset into $K$ different "folds" - usually $5$ or $10$. Say, in our example, that we use $K=6$ and divide our dataset into six different folds. First, we use the first fold as our <a href="testing-set">validation set</a>, train on the other five and compute the prediction error of the fitted model. Then, we use the second fold as our <a href="testing-set">validation set</a> and train on the remaining sets, this time also computing the prediction error. Doing this cycle through all $K=6$ folds and averaging the prediction errors gives us our <a href="cross-validation">cross-validation estimate</a> of the prediction error. </p>

<p>An illustration of this:</p>

<p><img src="https://raw.githubusercontent.com/qingkaikong/blog/master/2017_05_More_on_applying_ANN/figures/figure_1.jpg" alt="" /></p>

<p>Denoting the fitted function that excludes the $k$th fold as $\hat f^{-k}(X)$, and defining a function $\kappa:\{1,\dots,N\}\rightarrow\{1,\dots,K\}$ that takes in an index of the total dataset and returns the index of the fold containing that datapoint, we can write the <a href="cross-validation">cross-validation</a> estimate of the prediction error a</p>

<p>$$\text{CV}(\hat f) = \frac{1}{N}\sum_{i=1}^NL\left(y_i, \hat f^{-\kappa(i)}(x_i)\right).$$</p>

<p>With a set of models indexed by the tuning parameter $\alpha$, we see that</p>

<p>$$ \text{CV}(\hat f,\alpha) = \frac{1}{N}\sum_{i=1}^NL\left(y_i, \hat f^{-\kappa(i)}(x_i, \alpha)\right) $$</p>

<p>is the curve we need to find the minimum of. The value of $\alpha$ minimizing this curve is often denoted $\hat \alpha$, and we now choose our final model to be $f(x,\hat \alpha)$.</p>

<h4 id="choice-of-k">Choice of $K$</h4>

<p>There is no clear solution on how to choose $K$, but we have to be aware that there is a <a href="bias-variance-tradeoff">bias-variance tradeoff</a>. For example, setting $K=N$ (called <em>leave-one-out cross-validation</em> or <em>LOOCV</em>) estimates the expected test error approximatively unbiased. However, it has very large variance since the <a href="training-set">training sets</a> are very similar to each other.</p>

<h2 id="bootstrap">Bootstrap</h2>

<p>The <strong><a href="bootstrapping-statistics-">bootstrap method</a></strong> is (in general) a method of assessing <em>statistical accuracy</em> of a model. Briefly, it involves drawing random samples from our <a href="training-set">training dataset</a> <a href="sampling-with-replacement">with replacement</a>, and generating another dataset with the same size as our original <a href="training-set">training set</a>. Thereafter, we refit our model to this new bootstrap set. This is repeated a number of times, and the computed values from our bootstrap-models are used to estimate the conditional error of our <a href="training-set">training set</a>. </p>

<hr />

<p>Meta:</p>

<ul>
<li>Course: <a href="stk-in4300">STK-IN4300</a></li>
<li>References: <a href="https://web.stanford.edu/%7Ehastie/Papers/ESLII.pdf">The Elements of Statistical Mechanics</a> and <a href="riccardo-de-bin">Riccardo's</a> <a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/slides/lecture_3.pdf">lecture notes</a>.</li>
<li>Date: <a href="daily/2020-08-31">daily/2020-08-31</a> and <a href="daily/2020-09-02">daily/2020-09-02</a></li>
</ul>
</div>
    </article>
  </div>
  
  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"
    integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"
    integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous"></script>

  <!-- Parsing single dollar signs -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false}
        ]
    });
    });
  </script>
  
  <!-- Syntax highlighting through highlight.js -->
  <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/styles/default.min.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@10.4.0/highlight.min.js"></script>

  <script>
    // Ignore highlighting of mermaid
    hljs.configure({noHighlightRe: /^mermaid$/});
    hljs.initHighlightingOnLoad();
  </script>
</body>

</html>
